"""Evaluation tool for RAG system answers using LLM as Judge."""
import sys
from pathlib import Path

# Ensure project root is in path
if str(Path(__file__).parent.parent) not in sys.path:
    sys.path.insert(0, str(Path(__file__).parent.parent))

from typing import Dict, Optional, List, Tuple, Any
import json
import re
from langchain_core.documents import Document
from config.settings import OLLAMA_MODEL, OPENROUTER_API_KEY, OPENROUTER_BASE_URL, OPENROUTER_EVAL_MODEL
from pydantic import BaseModel, Field
from enum import Enum


def evaluate_answer(
    question: str,
    ground_truth: str,
    system_answer: str,
    choices: Optional[str] = None,
    retrieved_docs: Optional[List[Tuple[Document, float]]] = None
) -> Dict[str, str]:
    """
    Evaluate system answer against ground truth using LLM as Judge.
    
    Args:
        question: The question text
        ground_truth: The correct answer (ground truth)
        system_answer: The answer generated by the RAG system
        choices: Optional choices (for multiple choice questions)
        retrieved_docs: Optional list of retrieved documents with scores
        
    Returns:
        Dictionary with "evaluation" (Correct/Partially Correct/Incorrect) and "reason"
    """
    try:
        import ollama
        
        # Format retrieved snippets
        snippets_text = ""
        if retrieved_docs:
            snippet_parts = []
            for i, (doc, score) in enumerate(retrieved_docs[:5], 1):  # Max 5 documents
                doc_content = doc.page_content[:300]  # Max 300 chars per document
                snippet_parts.append(f"Snippet {i} (Similarity: {score:.2f}):\n{doc_content}")
            snippets_text = "\n".join(snippet_parts)
        
        # Build evaluation prompt
        evaluation_prompt = f"""You are an expert system designed to evaluate answers generated by a RAG (Retrieval-Augmented Generation) system. 
Follow these steps carefully:

STEP 1: Understand the following terms:
- LLM as Judge: You act as a judge that evaluates the correctness of answers.
- RAG-as: The system generates answers using retrieval of relevant documents plus language generation.
- Correctness: Determines if the system's answer matches the ground-truth answer (Correct, Partially Correct, Incorrect).
- Evaluation Dataset: A set of questions (text or images), the correct answers (ground-truth), and optionally retrieved context documents, used to measure system performance.

STEP 2: Evaluation Process
Given:
- Question: {question}
- Choices (if applicable): {choices if choices else "N/A"}
- Ground-truth answer: {ground_truth}
- System-generated answer: {system_answer}
- Retrieved context documents: {snippets_text if snippets_text else "N/A"}

Do the following:
1. Read and understand the question, choices, and context documents.
2. Compare the system answer with the ground-truth answer.
3. Determine correctness strictly:
   - "Correct" if it fully matches the ground-truth or is supported clearly by context.
   - "Partially Correct" if it is partially accurate or misses small details.
   - "Incorrect" if it is wrong or contradicts the context or ground-truth.
4. Provide a brief explanation (1 sentence max) for your judgment.

STEP 3: Output
Return **only** a JSON object in this exact format:
{{
  "evaluation": "Correct | Partially Correct | Incorrect",
  "reason": "brief explanation"
}}"""
        
        # Call Qwen3-VL:4b for evaluation
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": evaluation_prompt
                }
            ]
        )
        
        response_text = response["message"]["content"].strip()
        
        # Extract JSON from response
        try:
            # Search for JSON in response (may contain additional text)
            json_match = re.search(r'\{[^{}]*"evaluation"[^{}]*"reason"[^{}]*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                # Try to parse full response
                json_str = response_text
            
            result = json.loads(json_str)
            
            # Validate format
            if "evaluation" in result and "reason" in result:
                evaluation_value = result["evaluation"]
                # Normalize evaluation value
                if "correct" in evaluation_value.lower() and "partially" not in evaluation_value.lower() and "incorrect" not in evaluation_value.lower():
                    evaluation_value = "Correct"
                elif "partially" in evaluation_value.lower():
                    evaluation_value = "Partially Correct"
                elif "incorrect" in evaluation_value.lower():
                    evaluation_value = "Incorrect"
                
                return {
                    "evaluation": evaluation_value,
                    "reason": result["reason"]
                }
            else:
                return {
                    "evaluation": "Incorrect",
                    "reason": "Invalid evaluation format from LLM"
                }
                
        except json.JSONDecodeError as e:
            print(f"[EvaluationTool][ERROR] Failed to parse JSON response: {e}")
            print(f"[EvaluationTool][DEBUG] Response text: {response_text[:200]}")
            return {
                "evaluation": "Incorrect",
                "reason": f"Failed to parse evaluation response: {str(e)}"
            }
            
    except Exception as e:
        print(f"[EvaluationTool][ERROR] Evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        return {
            "evaluation": "Incorrect",
            "reason": f"Evaluation error: {str(e)}"
        }


def evaluate_multimodal_rag(
    query: str,
    retrieved_context: str,
    generated_answer: str
) -> Dict[str, Any]:
    """
    Evaluate Multimodal RAG system output using LLM as Judge with three criteria:
    1. Context Relevance (0-3)
    2. Answer Relevance (0-3)
    3. Groundedness (0-3)
    
    Args:
        query: The user's query/question
        retrieved_context: The retrieved multimodal context (text, image descriptions, diagrams, captions)
        generated_answer: The answer generated by the RAG system
        
    Returns:
        Dictionary with evaluation results including:
        - context_relevance: {"score": "0-3", "explanation": "..."}
        - answer_relevance: {"score": "0-3", "explanation": "..."}
        - groundedness: {"score": "0-3", "explanation": "..."}
    """
    try:
        import ollama
        
        # Build evaluation prompt
        evaluation_prompt = f"""You are a strict evaluator (LLM Judge) for an educational RAG system.

Evaluate the system output using THREE criteria:

1. Context Relevance:
How well does the retrieved context match the question?

2. Answer Relevance:
How well does the answer address the question?

3. Groundedness:
How faithful is the answer to the retrieved context?

SCORING:
"0" = Failed
"1" = Weak
"2" = Acceptable
"3" = Excellent

RULES:
- Use ONLY the retrieved context.
- Penalize unsupported claims.
- Do NOT improve or rewrite the answer.
- Be objective and concise.

INPUTS:
Question:
{query}

Retrieved Context:
{retrieved_context}

Generated Answer:
{generated_answer}

OUTPUT FORMAT:
Return ONLY a JSON object in this exact format:
{{
  "context_relevance": {{
    "score": "0" | "1" | "2" | "3",
    "explanation": "explanation + score"
  }},
  "answer_relevance": {{
    "score": "0" | "1" | "2" | "3",
    "explanation": "explanation + score"
  }},
  "groundedness": {{
    "score": "0" | "1" | "2" | "3",
    "explanation": "explanation + score"
  }}
}}"""
        
        # Call LLM for evaluation
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": evaluation_prompt
                }
            ]
        )
        
        response_text = response["message"]["content"].strip()
        
        # Extract JSON from response
        try:
            # Try to find JSON object in response (look for opening and closing braces)
            # Find the first { and match until the last }
            brace_start = response_text.find('{')
            brace_end = response_text.rfind('}')
            
            if brace_start != -1 and brace_end != -1 and brace_end > brace_start:
                json_str = response_text[brace_start:brace_end + 1]
            else:
                # Try to parse full response
                json_str = response_text
            
            result = json.loads(json_str)
            
            # Validate structure
            required_keys = ["context_relevance", "answer_relevance", "groundedness"]
            if all(key in result for key in required_keys):
                # Validate each criterion has score and explanation
                for key in required_keys:
                    if "score" not in result[key] or "explanation" not in result[key]:
                        raise ValueError(f"Missing 'score' or 'explanation' in {key}")
                    
                    # Ensure score is string and between 0-3
                    score = str(result[key]["score"]).strip()
                    if score not in ["0", "1", "2", "3"]:
                        # Try to normalize invalid scores
                        try:
                            score_int = int(score)
                            if score_int < 0:
                                score = "0"
                            elif score_int > 3:
                                score = "3"
                            else:
                                score = str(score_int)
                        except:
                            score = "0"
                        result[key]["score"] = score
                
                return result
            else:
                raise ValueError(f"Missing required keys. Found: {list(result.keys())}")
                
        except (json.JSONDecodeError, ValueError) as e:
            print(f"[EvaluationTool][ERROR] Failed to parse JSON response: {e}")
            print(f"[EvaluationTool][DEBUG] Response text: {response_text[:500]}")
            # Return default evaluation with error explanation
            return {
                "context_relevance": {
                    "score": "0",
                    "explanation": f"Failed to parse evaluation response: {str(e)}"
                },
                "answer_relevance": {
                    "score": "0",
                    "explanation": f"Failed to parse evaluation response: {str(e)}"
                },
                "groundedness": {
                    "score": "0",
                    "explanation": f"Failed to parse evaluation response: {str(e)}"
                }
            }
            
    except Exception as e:
        print(f"[EvaluationTool][ERROR] Multimodal RAG evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        # Return default evaluation with error explanation
        return {
            "context_relevance": {
                "score": "0",
                "explanation": f"Evaluation error: {str(e)}"
            },
            "answer_relevance": {
                "score": "0",
                "explanation": f"Evaluation error: {str(e)}"
            },
            "groundedness": {
                "score": "0",
                "explanation": f"Evaluation error: {str(e)}"
            }
        }


def format_retrieved_context_for_evaluation(
    retrieved_docs: Optional[List[Tuple[Document, float]]] = None,
    max_docs: int = 10,
    max_chars_per_doc: int = 500
) -> Tuple[str, str]:
    """
    Format retrieved documents for evaluation context, separating text and image captions.
    
    Args:
        retrieved_docs: List of (Document, similarity_score) tuples
        max_docs: Maximum number of documents to include
        max_chars_per_doc: Maximum characters per document
        
    Returns:
        Tuple of (text_context, image_captions) strings
    """
    if not retrieved_docs:
        return "No context retrieved.", ""
    
    text_parts = []
    image_captions = []
    
    for i, (doc, score) in enumerate(retrieved_docs[:max_docs], 1):
        content = doc.page_content[:max_chars_per_doc]
        metadata_str = ""
        has_image = False
        
        if doc.metadata:
            # Check if document has image-related metadata
            if doc.metadata.get("image_path") or doc.metadata.get("visual_description") or doc.metadata.get("ocr_text"):
                has_image = True
                # Extract image-related information
                img_info = []
                if doc.metadata.get("visual_description"):
                    img_info.append(f"Visual Description: {doc.metadata.get('visual_description')[:200]}")
                if doc.metadata.get("ocr_text"):
                    img_info.append(f"OCR Text: {doc.metadata.get('ocr_text')[:200]}")
                if img_info:
                    image_captions.append(f"[Image {i}] (Similarity: {score:.3f}):\n" + "\n".join(img_info))
            
            # Include other metadata (exclude large fields)
            metadata_items = []
            for k, v in doc.metadata.items():
                if k not in ["image_path", "image_base64", "embeddings", "visual_description", "ocr_text"]:
                    metadata_items.append(f"{k}: {v}")
            if metadata_items:
                metadata_str = f" (Metadata: {', '.join(metadata_items)})"
        
        # Add to text context if not primarily an image
        if not has_image or content.strip():
            text_parts.append(
                f"[Document {i}] (Similarity: {score:.3f}){metadata_str}:\n{content}"
            )
    
    text_context = "\n\n".join(text_parts) if text_parts else "No text context retrieved."
    image_context = "\n\n".join(image_captions) if image_captions else ""
    
    return text_context, image_context


def evaluation_function(question: str, ground_truth: str, system_answer: str, choices: str = None) -> str:
    """
    Wrapper function for LangChain tool.
    
    Returns:
        String representation of evaluation result
    """
    result = evaluate_answer(question, ground_truth, system_answer, choices)
    return json.dumps(result, ensure_ascii=False)


# ==================== Mistral AI Evaluation with Structured Outputs ====================

# Define Enum for scores
class Score(str, Enum):
    no_relevance = "0"
    low_relevance = "1"
    medium_relevance = "2"
    high_relevance = "3"


# Define a constant for the score description
SCORE_DESCRIPTION = (
    "Score as a string between '0' and '3'. "
    "0: No relevance/Not grounded/Irrelevant - The context/answer is completely unrelated or not based on the context. "
    "1: Low relevance/Low groundedness/Somewhat relevant - The context/answer has minimal relevance or grounding. "
    "2: Medium relevance/Medium groundedness/Mostly relevant - The context/answer is somewhat relevant or grounded. "
    "3: High relevance/High groundedness/Fully relevant - The context/answer is highly relevant or grounded."
)


# Define separate classes for each criterion with detailed descriptions
class ContextRelevance(BaseModel):
    explanation: str = Field(..., description=("Step-by-step reasoning explaining how the retrieved context aligns with the user's query. "
                    "Consider the relevance of the information to the query's intent and the appropriateness of the context "
                    "in providing a coherent and useful response."))
    score: Score = Field(..., description=SCORE_DESCRIPTION)


class AnswerRelevance(BaseModel):
    explanation: str = Field(..., description=("Step-by-step reasoning explaining how well the generated answer addresses the user's original query. "
                    "Consider the helpfulness and on-point nature of the answer, aligning with the user's intent and providing valuable insights."))
    score: Score = Field(..., description=SCORE_DESCRIPTION)


class Groundedness(BaseModel):
    explanation: str = Field(..., description=("Step-by-step reasoning explaining how faithful the generated answer is to the retrieved context. "
                    "Consider the factual accuracy and reliability of the answer, ensuring it is grounded in the retrieved information."))
    score: Score = Field(..., description=SCORE_DESCRIPTION)


class RAGEvaluation(BaseModel):
    context_relevance: ContextRelevance = Field(..., description="Evaluation of the context relevance to the query, considering how well the retrieved context aligns with the user's intent.")
    answer_relevance: AnswerRelevance = Field(..., description="Evaluation of the answer relevance to the query, assessing how well the generated answer addresses the user's original query.")
    groundedness: Groundedness = Field(..., description="Evaluation of the groundedness of the generated answer, ensuring it is faithful to the retrieved context.")


def evaluate_rag_with_mistral(
    query: str,
    retrieved_context: str,
    generated_answer: str,
    image_captions: str = ""
) -> Dict[str, Any]:
    """
    Evaluate RAG system output using Qwen 2.5-VL-7B-Instruct (OpenRouter) with structured JSON outputs.
    
    This function uses Qwen 2.5-VL-7B-Instruct through OpenRouter to evaluate:
    1. Context Relevance (0-3)
    2. Answer Relevance (0-3)
    3. Groundedness (0-3)
    
    Supports both text and image inputs (multimodal evaluation).
    
    Args:
        query: The user's query/question
        retrieved_context: The retrieved context (formatted string, may include image descriptions)
        generated_answer: The answer generated by the RAG system
        image_captions: Image captions/descriptions if available
        
    Returns:
        Dictionary with evaluation results including:
        - context_relevance: {"score": "0-3", "explanation": "..."}
        - answer_relevance: {"score": "0-3", "explanation": "..."}
        - groundedness: {"score": "0-3", "explanation": "..."}
    """
    try:
        import requests
        
        # Build comprehensive evaluation prompt
        evaluation_prompt = f"""You are an expert judge for evaluating answers in an educational search system. You may receive:
- Text input only
- Image input only (represented as a descriptive caption)
- Both text and image inputs together

Your task is to evaluate the system-generated answer based on the following criteria:

1. Context Relevance: How well does the retrieved context (text and/or image captions) support the generated answer? Score 0-3.
2. Answer Relevance: How well does the generated answer address the user's question? Score 0-3.
3. Groundedness: Is the generated answer faithful to the retrieved context (and any image captions)? Score 0-3.

Evaluation Guidelines:
- 0 = Not relevant / Not grounded
- 1 = Low relevance / Low grounding
- 2 = Medium relevance / Mostly grounded
- 3 = High relevance / Fully grounded
- Consider factual correctness, completeness, and clarity of reasoning.
- If an image is included, consider whether the answer correctly interprets the visual information.

Output Format (strict JSON):
{{
  "context_relevance": {{"score": 0-3, "explanation": "Explain how well the context supports the answer."}},
  "answer_relevance": {{"score": 0-3, "explanation": "Explain how well the answer addresses the question."}},
  "groundedness": {{"score": 0-3, "explanation": "Explain how faithful the answer is to the provided context."}}
}}

Here is the information to evaluate:

Question: {query}
Retrieved Text Context: {retrieved_context}
Retrieved Image Captions: {image_captions if image_captions else '(No image captions)'}
Generated Answer: {generated_answer}

Now provide your evaluation in the JSON format specified above:"""
        
        # Check if API key is available
        if not OPENROUTER_API_KEY:
            raise ValueError("OPENROUTER_API_KEY is not set. Please set it in your .env file or environment variables.")
        
        # Call Qwen 2.5-VL-7B-Instruct through OpenRouter
        headers = {
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/your-repo",  # Optional: for tracking
            "X-Title": "RAG Evaluation System"  # Optional: for tracking
        }
        
        payload = {
            "model": OPENROUTER_EVAL_MODEL,
            "messages": [
                {
                    "role": "user",
                    "content": evaluation_prompt
                }
            ],
            "temperature": 0,  # Use deterministic output for evaluation
            "max_tokens": 2000
        }
        
        response = requests.post(
            f"{OPENROUTER_BASE_URL}/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        
        response.raise_for_status()
        response_data = response.json()
        
        response_text = response_data["choices"][0]["message"]["content"].strip()
        
        # Extract JSON from response
        try:
            # Try to find JSON object in response (look for opening and closing braces)
            brace_start = response_text.find('{')
            brace_end = response_text.rfind('}')
            
            if brace_start != -1 and brace_end != -1 and brace_end > brace_start:
                json_str = response_text[brace_start:brace_end + 1]
            else:
                # Try to parse full response
                json_str = response_text
            
            result = json.loads(json_str)
            
            # Validate structure
            required_keys = ["context_relevance", "answer_relevance", "groundedness"]
            if all(key in result for key in required_keys):
                # Validate each criterion has score and explanation
                for key in required_keys:
                    if "score" not in result[key] or "explanation" not in result[key]:
                        raise ValueError(f"Missing 'score' or 'explanation' in {key}")
                    
                    # Ensure score is between 0-3
                    score = result[key]["score"]
                    if isinstance(score, str):
                        score = int(score) if score.isdigit() else 0
                    score = max(0, min(3, int(score)))  # Clamp between 0-3
                    result[key]["score"] = str(score)
                
                return result
            else:
                raise ValueError(f"Missing required keys. Found: {list(result.keys())}")
                
        except (json.JSONDecodeError, ValueError) as e:
            print(f"[EvaluationTool][ERROR] Failed to parse JSON response: {e}")
            print(f"[EvaluationTool][DEBUG] Response text: {response_text[:500]}")
            # Return default evaluation with error explanation
            return {
                "context_relevance": {
                    "score": "0",
                    "explanation": f"Failed to parse evaluation response: {str(e)}"
                },
                "answer_relevance": {
                    "score": "0",
                    "explanation": f"Failed to parse evaluation response: {str(e)}"
                },
                "groundedness": {
                    "score": "0",
                    "explanation": f"Failed to parse evaluation response: {str(e)}"
                }
            }
            
    except ImportError:
        print("[EvaluationTool][ERROR] requests package not installed. Install it with: pip install requests")
        raise
    except requests.exceptions.RequestException as e:
        print(f"[EvaluationTool][ERROR] OpenRouter API request failed: {e}")
        if hasattr(e, 'response') and e.response is not None:
            print(f"[EvaluationTool][DEBUG] Response status: {e.response.status_code}")
            print(f"[EvaluationTool][DEBUG] Response text: {e.response.text[:500]}")
        raise
    except Exception as e:
        print(f"[EvaluationTool][ERROR] RAG evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        # Return default evaluation with error explanation
        return {
            "context_relevance": {
                "score": "0",
                "explanation": f"Evaluation error: {str(e)}"
            },
            "answer_relevance": {
                "score": "0",
                "explanation": f"Evaluation error: {str(e)}"
            },
            "groundedness": {
                "score": "0",
                "explanation": f"Evaluation error: {str(e)}"
            }
        }







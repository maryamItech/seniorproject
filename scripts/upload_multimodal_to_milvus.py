"""Upload multimodal embeddings (text + image) to Milvus Cloud.

This script:
- Reads multimodal_embeddings.json (generated by generate_multimodal_embeddings.py)
- Creates a new Milvus collection for multimodal data
- Stores both text_embedding and image_embedding vectors
- Links them by ID with metadata (text, image_path, question, etc.)

Collection Schema:
- id (VARCHAR, primary key): Record ID
- text_embedding (FLOAT_VECTOR, dim=768): Text embedding from Visualized_BGE
- image_embedding (FLOAT_VECTOR, dim=768): Image embedding from Visualized_BGE
- text (VARCHAR): Combined text content
- image_path (VARCHAR): Path to image file
- question (VARCHAR): Original question
- answer (VARCHAR): Answer
- split (VARCHAR): Dataset split (train/validation/test)
"""

import sys
from pathlib import Path
import json
import logging
from typing import List, Dict
from tqdm import tqdm

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from pymilvus import MilvusClient, DataType
from config.settings import (
    MILVUS_URI,
    MILVUS_TOKEN,
    MILVUS_MULTIMODAL_COLLECTION_NAME,
    EMBEDDINGS_DIR,
)

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger(__name__)


def upload_multimodal_embeddings(
    json_path: Path,
    collection_name: str = MILVUS_MULTIMODAL_COLLECTION_NAME,
    batch_size: int = 1000,
    drop_existing: bool = False,
) -> None:
    """Upload multimodal embeddings to Milvus.
    
    Args:
        json_path: Path to multimodal_embeddings.json
        collection_name: Name of Milvus collection
        batch_size: Number of records to insert per batch
        drop_existing: If True, drop existing collection before creating new one
    """
    
    # Load JSON data
    logger.info(f"Loading embeddings from {json_path}")
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    if not data:
        logger.error("No data found in JSON file")
        return
    
    logger.info(f"Loaded {len(data)} records")
    
    # Check embedding dimensions
    sample = data[0]
    text_dim = len(sample.get("text_embedding", []))
    image_dim = len(sample.get("image_embedding", []))
    
    if text_dim != 768 or image_dim != 768:
        logger.warning(f"Unexpected dimensions: text={text_dim}, image={image_dim}. Expected 768 for both.")
    
    logger.info(f"Text embedding dimension: {text_dim}")
    logger.info(f"Image embedding dimension: {image_dim}")
    
    # Connect to Milvus
    logger.info(f"Connecting to Milvus at {MILVUS_URI}")
    client = MilvusClient(uri=MILVUS_URI, token=MILVUS_TOKEN, timeout=60)
    
    # Check if collection exists
    if client.has_collection(collection_name):
        if drop_existing:
            logger.info(f"Collection '{collection_name}' exists. Dropping it...")
            client.drop_collection(collection_name)
        else:
            logger.warning(f"Collection '{collection_name}' already exists. Use --drop-existing to replace it.")
            return
    
    # Create collection schema
    logger.info(f"Creating collection '{collection_name}'...")
    schema = client.create_schema(auto_id=False, enable_dynamic_field=False)
    
    # Primary key: id (VARCHAR)
    schema.add_field("id", DataType.VARCHAR, is_primary=True, max_length=64)
    
    # Text embedding vector (768 dim)
    schema.add_field("text_embedding", DataType.FLOAT_VECTOR, dim=text_dim)
    
    # Image embedding vector (768 dim)
    schema.add_field("image_embedding", DataType.FLOAT_VECTOR, dim=image_dim)
    
    # Metadata fields
    schema.add_field("text", DataType.VARCHAR, max_length=4096)
    schema.add_field("image_path", DataType.VARCHAR, max_length=512)
    schema.add_field("question", DataType.VARCHAR, max_length=2048)
    schema.add_field("answer", DataType.VARCHAR, max_length=512)
    schema.add_field("choices", DataType.VARCHAR, max_length=2048)
    schema.add_field("explanation", DataType.VARCHAR, max_length=4096)
    schema.add_field("split", DataType.VARCHAR, max_length=32)
    
    # Create index for text_embedding (for text search)
    index_params = client.prepare_index_params()
    index_params.add_index(
        field_name="text_embedding",
        index_type="AUTOINDEX",
        metric_type="COSINE",
    )
    
    # Create index for image_embedding (for image search)
    index_params.add_index(
        field_name="image_embedding",
        index_type="AUTOINDEX",
        metric_type="COSINE",
    )
    
    client.create_collection(
        collection_name=collection_name,
        schema=schema,
        index_params=index_params,
        consistency_level="Bounded",
    )
    logger.info(f"Collection '{collection_name}' created successfully")
    
    # Prepare data for insertion
    logger.info("Preparing data for insertion...")
    entities = []
    
    for record in tqdm(data, desc="Preparing entities"):
        entity = {
            "id": str(record["id"]),
            "text_embedding": record["text_embedding"],
            "image_embedding": record["image_embedding"],
            "text": str(record.get("text", ""))[:4096],  # Truncate if too long
            "image_path": str(record.get("image_path", ""))[:512],
            "question": str(record.get("question", ""))[:2048],
            "answer": str(record.get("answer", ""))[:512],
            "choices": str(record.get("choices", ""))[:2048],
            "explanation": str(record.get("explanation", ""))[:4096],
            "split": str(record.get("split", ""))[:32],
        }
        entities.append(entity)
    
    # Insert data in batches
    logger.info(f"Inserting {len(entities)} records in batches of {batch_size}...")
    total_inserted = 0
    
    for i in tqdm(range(0, len(entities), batch_size), desc="Inserting batches"):
        batch = entities[i:i + batch_size]
        try:
            insert_result = client.insert(collection_name=collection_name, data=batch)
            total_inserted += len(batch)
            logger.debug(f"Inserted batch {i // batch_size + 1}: {len(batch)} records")
        except Exception as exc:
            logger.error(f"Error inserting batch {i // batch_size + 1}: {exc}")
            continue
    
    # Flush to ensure data is persisted
    logger.info("Flushing collection...")
    client.flush(collection_name)
    
    # Get collection stats
    stats = client.get_collection_stats(collection_name)
    logger.info(f"Collection stats: {stats}")
    
    logger.info(f"\nâœ… Upload complete!")
    logger.info(f"  Total records inserted: {total_inserted}")
    logger.info(f"  Collection name: {collection_name}")
    logger.info(f"  Text embedding dimension: {text_dim}")
    logger.info(f"  Image embedding dimension: {image_dim}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Upload multimodal embeddings to Milvus")
    parser.add_argument(
        "--json-path",
        type=str,
        default=str(EMBEDDINGS_DIR / "multimodal_embeddings.json"),
        help="Path to multimodal_embeddings.json file"
    )
    parser.add_argument(
        "--collection-name",
        type=str,
        default=MILVUS_MULTIMODAL_COLLECTION_NAME,
        help="Milvus collection name"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="Number of records per batch"
    )
    parser.add_argument(
        "--drop-existing",
        action="store_true",
        help="Drop existing collection if it exists"
    )
    
    args = parser.parse_args()
    
    upload_multimodal_embeddings(
        json_path=Path(args.json_path),
        collection_name=args.collection_name,
        batch_size=args.batch_size,
        drop_existing=args.drop_existing,
    )


if __name__ == "__main__":
    main()



